import os
import re
import json
import requests
from urllib.parse import urljoin, urlparse
from concurrent.futures import ThreadPoolExecutor, as_completed
from time import sleep, time
from playwright.async_api import async_playwright
import sys
import asyncio
from tqdm import tqdm
import aiohttp
import aiofiles
from typing import List, Dict, Optional, Tuple
from fastapi import FastAPI, HTTPException, Query, BackgroundTasks
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel, HttpUrl
import uvicorn
from contextlib import asynccontextmanager
import hashlib
from fastapi.staticfiles import StaticFiles

BASE_URL = "https://webfandom.ru"
HEADERS = {
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
    "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8",
    "Accept-Language": "ru-RU,ru;q=0.8,en-US;q=0.5,en;q=0.3",
    "Accept-Encoding": "gzip, deflate, br",
    "Connection": "keep-alive",
    "Upgrade-Insecure-Requests": "1"
}

# –ì–ª–æ–±–∞–ª—å–Ω—ã–π –∫–µ—à –¥–ª—è —Ö—Ä–∞–Ω–µ–Ω–∏—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ –º–∞–Ω–≥–µ
manga_cache = {}
browser_pool = None

class MangaRequest(BaseModel):
    url: HttpUrl
    max_chapters: Optional[int] = None

class MangaResponse(BaseModel):
    title: str
    alternative_titles: Dict[str, str] = {}
    description: str
    genres: List[str] = []
    cover_url: Optional[str] = None
    local_cover_path: Optional[str] = None
    additional_info: Dict = {}
    chapters: List[Dict] = []
    total_chapters: int
    source_url: str
    manga_id: str

class ChapterResponse(BaseModel):
    chapter_id: str
    name: str
    pages: List[str] = []
    total_pages: int
    download_status: str

@asynccontextmanager
async def lifespan(app: FastAPI):
    # Startup
    global browser_pool
    print("üöÄ –ó–∞–ø—É—Å–∫ —Å–µ—Ä–≤–µ—Ä–∞ –ø–∞—Ä—Å–µ—Ä–∞ –º–∞–Ω–≥–∏...")
    browser_pool = await async_playwright().start()
    yield
    # Shutdown
    print("üõë –û—Å—Ç–∞–Ω–æ–≤–∫–∞ —Å–µ—Ä–≤–µ—Ä–∞...")
    if browser_pool:
        await browser_pool.stop()

app = FastAPI(
    title="Manga Parser API",
    description="API –¥–ª—è –ø–∞—Ä—Å–∏–Ω–≥–∞ –º–∞–Ω–≥–∏ —Å WebFandom.ru",
    version="1.0.0",
    lifespan=lifespan
)

os.makedirs("manga", exist_ok=True)

# –†–∞–∑–¥–∞—ë–º —Ñ–∞–π–ª—ã –∏–∑ –ø–∞–ø–∫–∏ "manga" –ø–æ –∞–¥—Ä–µ—Å—É /static
app.mount("/static", StaticFiles(directory="manga"), name="static")

# üëá –†–∞–∑—Ä–µ—à–∞–µ–º —Ñ—Ä–æ–Ω—Ç—É –æ–±—Ä–∞—â–∞—Ç—å—Å—è –∫ API
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],  # –¥–ª—è –æ—Ç–ª–∞–¥–∫–∏ ‚Äî –º–æ–∂–Ω–æ –ø–æ—Ç–æ–º –æ–≥—Ä–∞–Ω–∏—á–∏—Ç—å ["http://localhost:5173"]
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

class FastMangaParser:
    def __init__(self, max_workers: int = 10):
        self.max_workers = max_workers
        
    def sanitize_filename(self, name: str) -> str:
        """–û—á–∏—Å—Ç–∫–∞ –∏–º–µ–Ω–∏ —Ñ–∞–π–ª–∞ –æ—Ç –Ω–µ–¥–æ–ø—É—Å—Ç–∏–º—ã—Ö —Å–∏–º–≤–æ–ª–æ–≤"""
        return re.sub(r'[\\/*?:"<>|]', "_", name).strip()[:100]
    
    def get_manga_id(self, url: str) -> str:
        """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º —É–Ω–∏–∫–∞–ª—å–Ω—ã–π ID –¥–ª—è –º–∞–Ω–≥–∏ –Ω–∞ –æ—Å–Ω–æ–≤–µ URL"""
        return hashlib.md5(url.encode()).hexdigest()
    
    async def download_image_async(self, session: aiohttp.ClientSession, url: str, path: str, retries: int = 3) -> bool:
        """–ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–æ–µ —Å–∫–∞—á–∏–≤–∞–Ω–∏–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è"""
        if os.path.exists(path):
            return True
        if url.startswith("/"):
             url = urljoin(BASE_URL, url)
        
        headers = {
            **HEADERS,
            "Referer": BASE_URL,
            "Accept": "image/webp,image/apng,image/svg+xml,image/*,*/*;q=0.8"
        }
        
        for attempt in range(retries):
            try:
                async with session.get(url, headers=headers, timeout=30) as response:
                    if response.status == 200:
                        content = await response.read()
                        os.makedirs(os.path.dirname(path), exist_ok=True)
                        async with aiofiles.open(path, 'wb') as f:
                            await f.write(content)
                        return True
            except Exception as e:
                if attempt == retries - 1:
                    print(f"[WARN] –ù–µ —É–¥–∞–ª–æ—Å—å —Å–∫–∞—á–∞—Ç—å {url}: {e}")
                await asyncio.sleep(0.5)
        return False
    
    async def download_images_batch(self, img_urls: List[Tuple[str, str]]) -> int:
        """–ü–∞–∫–µ—Ç–Ω–∞—è –∑–∞–≥—Ä—É–∑–∫–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π"""
        connector = aiohttp.TCPConnector(limit=self.max_workers, force_close=True)
        timeout = aiohttp.ClientTimeout(total=300)
        
        async with aiohttp.ClientSession(connector=connector, timeout=timeout) as session:
            tasks = []
            for url, path in img_urls:
                task = self.download_image_async(session, url, path)
                tasks.append(task)
            
            results = await asyncio.gather(*tasks)
            return sum(results)
    
    async def get_full_manga_info(self, page) -> Dict:
        """–ü–æ–ª—É—á–∞–µ–º –ø–æ–ª–Ω—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –º–∞–Ω–≥–µ"""
        print("–ò–∑–≤–ª–µ–∫–∞–µ–º –ø–æ–ª–Ω—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –º–∞–Ω–≥–µ...")
        
        # –ñ–¥–µ–º –ø–æ—è–≤–ª–µ–Ω–∏—è –æ—Å–Ω–æ–≤–Ω–æ–≥–æ –∫–æ–Ω—Ç–µ–Ω—Ç–∞
        try:
            await page.wait_for_selector('h1, [data-testid="title"], .title, .manga-title', timeout=10000)
            await asyncio.sleep(2)
        except:
            print("–ü—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏–µ: –Ω–µ —É–¥–∞–ª–æ—Å—å –¥–æ–∂–¥–∞—Ç—å—Å—è –ø–æ–ª–Ω–æ–π –∑–∞–≥—Ä—É–∑–∫–∏, –ø—Ä–æ–¥–æ–ª–∂–∞–µ–º...")
            await asyncio.sleep(1)
        
        # –ü—Ä–æ–±—É–µ–º —Ä–∞–∑–≤–µ—Ä–Ω—É—Ç—å –≤—Å–µ —Ç–µ–≥–∏
        try:
            print("–†–∞–∑–≤–æ—Ä–∞—á–∏–≤–∞–µ–º –≤—Å–µ —Ç–µ–≥–∏...")
            await page.evaluate("""
                () => {
                    const showMoreButtons = document.querySelectorAll('button, span, div');
                    showMoreButtons.forEach(element => {
                        const text = element.textContent || '';
                        if (text.includes('–ü–æ–∫–∞–∑–∞—Ç—å –≤—Å–µ') || 
                            text.includes('...') || 
                            element.className.includes('show-more') ||
                            element.className.includes('expand')) {
                            try {
                                element.click();
                            } catch(e) {}
                        }
                    });
                    
                    const badges = document.querySelectorAll('.badge');
                    badges.forEach(badge => {
                        if (badge.textContent && badge.textContent.includes('–ü–æ–∫–∞–∑–∞—Ç—å –≤—Å–µ')) {
                            try {
                                badge.click();
                            } catch(e) {}
                        }
                    });
                }
            """)
            await asyncio.sleep(1)
        except:
            print("–ù–µ —É–¥–∞–ª–æ—Å—å —Ä–∞–∑–≤–µ—Ä–Ω—É—Ç—å —Ç–µ–≥–∏, –ø—Ä–æ–¥–æ–ª–∂–∞–µ–º...")
        
        # –ò–∑–≤–ª–µ–∫–∞–µ–º –¥–∞–Ω–Ω—ã–µ
        info = await page.evaluate(r"""
            () => {
                const data = {};
                
                // –ù–∞–∑–≤–∞–Ω–∏–µ –Ω–∞ —Ä—É—Å—Å–∫–æ–º
                const titleEl = document.querySelector('h1, [data-testid="title"], .title, .manga-title');
                data.title = titleEl ? titleEl.textContent.trim() : '–ë–µ–∑ –Ω–∞–∑–≤–∞–Ω–∏—è';
                
                // –ê–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω—ã–µ –Ω–∞–∑–≤–∞–Ω–∏—è
                data.alternative_titles = {};
                
                // –ò—â–µ–º –±–ª–æ–∫ —Å –∞–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω—ã–º–∏ –Ω–∞–∑–≤–∞–Ω–∏—è–º–∏
                const infoBlocks = document.querySelectorAll('.publication-info > div, .manga-info > div, .info-block, div');
                infoBlocks.forEach(block => {
                    const text = block.textContent || '';
                    
                    if (text.includes('–ê–Ω–≥–ª–∏–π—Å–∫–æ–µ –Ω–∞–∑–≤–∞–Ω–∏–µ:') || text.includes('English:')) {
                        const match = text.match(/(?:–ê–Ω–≥–ª–∏–π—Å–∫–æ–µ –Ω–∞–∑–≤–∞–Ω–∏–µ:|English:)\s*(.+?)(?:\n|$)/);
                        if (match) data.alternative_titles.english = match[1].trim();
                    }
                    
                    if (text.includes('–ö–æ—Ä–µ–π—Å–∫–æ–µ –Ω–∞–∑–≤–∞–Ω–∏–µ:') || text.includes('Korean:')) {
                        const match = text.match(/(?:–ö–æ—Ä–µ–π—Å–∫–æ–µ –Ω–∞–∑–≤–∞–Ω–∏–µ:|Korean:)\s*(.+?)(?:\n|$)/);
                        if (match) data.alternative_titles.korean = match[1].trim();
                    }
                    
                    if (text.includes('–Ø–ø–æ–Ω—Å–∫–æ–µ –Ω–∞–∑–≤–∞–Ω–∏–µ:') || text.includes('Japanese:')) {
                        const match = text.match(/(?:–Ø–ø–æ–Ω—Å–∫–æ–µ –Ω–∞–∑–≤–∞–Ω–∏–µ:|Japanese:)\s*(.+?)(?:\n|$)/);
                        if (match) data.alternative_titles.japanese = match[1].trim();
                    }
                });
                
                // –ü–æ–∏—Å–∫ –æ–±–ª–æ–∂–∫–∏
                let coverUrl = null;
                
                const pictureElement = document.querySelector('picture');
                if (pictureElement) {
                    const imgInPicture = pictureElement.querySelector('img');
                    if (imgInPicture && imgInPicture.src && !imgInPicture.src.startsWith('data:')) {
                        coverUrl = imgInPicture.src;
                    }
                }
                
                if (!coverUrl) {
                    const imgSelectors = [
                        'img[class*="rounded"]',
                        'img[alt*="–æ–±–ª–æ–∂–∫–∞"]',
                        'img[alt*="cover"]',
                        '.cover img',
                        '.manga-cover img',
                        'div.relative img',
                        '.publication-cover img',
                        'img.w-full'
                    ];
                    
                    for (const sel of imgSelectors) {
                        try {
                            const el = document.querySelector(sel);
                            if (el && el.src && 
                                !el.src.startsWith('data:') && 
                                !el.src.includes('avatar') && 
                                !el.src.includes('logo') &&
                                !el.src.includes('icon')) {
                                coverUrl = el.src;
                                break;
                            }
                        } catch(e) {}
                    }
                }
                
                if (!coverUrl) {
                    const imgs = Array.from(document.querySelectorAll('img'));
                    const bigImg = imgs.find(img => 
                        img.src && 
                        !img.src.startsWith('data:') &&
                        img.naturalWidth > 200 && 
                        img.naturalHeight > 300 &&
                        !img.src.includes('avatar') &&
                        !img.src.includes('logo')
                    );
                    if (bigImg) coverUrl = bigImg.src;
                }
                
                data.cover_url = coverUrl;
                
                // –û–ø–∏—Å–∞–Ω–∏–µ
                let description = '';
                const descSelectors = [
                    '.publication-description',
                    '.whitespace-pre-wrap',
                    '.description',
                    '.manga-description',
                    '[class*="description"]',
                    'div.font-light'
                ];
                
                for (const sel of descSelectors) {
                    try {
                        const el = document.querySelector(sel);
                        if (el && el.textContent && el.textContent.length > 50) {
                            description = el.textContent.trim();
                            break;
                        }
                    } catch(e) {}
                }
                
                data.description = description || '–û–ø–∏—Å–∞–Ω–∏–µ –æ—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç';
                
                // –°–æ–±–∏—Ä–∞–µ–º –í–°–ï —Ç–µ–≥–∏
                const allTags = new Set();
                
                const tagSelectors = [
                    'a .badge.text-wf-yellow',
                    'a .badge',
                    '.badge',
                    '.genre',
                    '.tag',
                    'a[href*="/catalog?genres"]',
                    'a[href*="/catalog?tags"]',
                    '.genres a',
                    '.tags a',
                    '[class*="badge"]:not([class*="show"])'
                ];
                
                tagSelectors.forEach(sel => {
                    try {
                        document.querySelectorAll(sel).forEach(el => {
                            let text = el.textContent.trim();
                            
                            if (text && 
                                text.length > 1 && 
                                text !== '...' && 
                                !text.includes('–ü–æ–∫–∞–∑–∞—Ç—å –≤—Å–µ') &&
                                !text.includes('–°–∫—Ä—ã—Ç—å') &&
                                !text.includes('–°–≤–µ—Ä–Ω—É—Ç—å')) {
                                
                                const parentLink = el.closest('a');
                                if (parentLink && parentLink.href && parentLink.href.includes('/catalog')) {
                                    text = parentLink.textContent.trim();
                                }
                                
                                if (text && !text.includes('–ü–æ–∫–∞–∑–∞—Ç—å')) {
                                    allTags.add(text);
                                }
                            }
                        });
                    } catch(e) {}
                });
                
                try {
                    document.querySelectorAll('a[href*="/catalog"]').forEach(link => {
                        const badge = link.querySelector('.badge');
                        if (badge) {
                            const text = badge.textContent.trim();
                            if (text && !text.includes('–ü–æ–∫–∞–∑–∞—Ç—å') && text !== '...') {
                                allTags.add(text);
                            }
                        }
                    });
                } catch(e) {}
                
                data.genres = Array.from(allTags);
                
                // –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è
                data.additional_info = {};
                
                try {
                    const allElements = document.querySelectorAll('*');
                    allElements.forEach(el => {
                        const text = el.textContent || '';
                        if (text.includes('–°—Ç–∞—Ç—É—Å')) {
                            if (text.includes('–ó–∞–≤–µ—Ä—à–µ–Ω')) data.additional_info.status = '–ó–∞–≤–µ—Ä—à–µ–Ω';
                            else if (text.includes('–ü—Ä–æ–¥–æ–ª–∂–∞–µ—Ç—Å—è')) data.additional_info.status = '–ü—Ä–æ–¥–æ–ª–∂–∞–µ—Ç—Å—è';
                            else if (text.includes('–ó–∞–º–æ—Ä–æ–∂–µ–Ω')) data.additional_info.status = '–ó–∞–º–æ—Ä–æ–∂–µ–Ω';
                        }
                        
                        if (text.includes('–ê–≤—Ç–æ—Ä')) {
                            const authorMatch = text.match(/–ê–≤—Ç–æ—Ä[:\s]+(.+?)(?:\n|$)/);
                            if (authorMatch) data.additional_info.author = authorMatch[1].trim();
                        }
                        
                        if (text.includes('–•—É–¥–æ–∂–Ω–∏–∫')) {
                            const artistMatch = text.match(/–•—É–¥–æ–∂–Ω–∏–∫[:\s]+(.+?)(?:\n|$)/);
                            if (artistMatch) data.additional_info.artist = artistMatch[1].trim();
                        }
                        
                        if (text.includes('–ì–æ–¥ –≤—ã–ø—É—Å–∫–∞') || text.includes('–ì–æ–¥')) {
                            const yearMatch = text.match(/\d{4}/);
                            if (yearMatch) data.additional_info.year = parseInt(yearMatch[0]);
                        }
                    });
                } catch(e) {}
                
                return data;
            }
        """)
        
        return info

    async def extract_images_from_chapter(self, page) -> List[str]:
        """–ò–∑–≤–ª–µ–∫–∞–µ–º –í–°–ï –∫–∞—Ä—Ç–∏–Ω–∫–∏ –∏–∑ –≥–ª–∞–≤—ã (Nuxt + img + data-* + scroll)"""
        img_urls = await page.evaluate(r"""
            () => {
                const images = [];

                // –ü—Ä–æ–≤–µ—Ä—è–µ–º –≥–ª–æ–±–∞–ª—å–Ω—ã–µ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ
                if (window.images) return window.images;
                if (window.chapterImages) return window.chapterImages;
                if (window.pageImages) return window.pageImages;

                // –ò—â–µ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –≤ Nuxt data
                if (window.__NUXT__ && window.__NUXT__.data) {
                    const findImages = (obj, depth = 0) => {
                        if (depth > 10) return [];
                        const imgs = [];
                        if (typeof obj === 'string' && obj.match(/\.(jpg|jpeg|png|webp)/i)) {
                            imgs.push(obj);
                        } else if (Array.isArray(obj)) {
                            obj.forEach(item => imgs.push(...findImages(item, depth + 1)));
                        } else if (typeof obj === 'object' && obj !== null) {
                            Object.values(obj).forEach(val => imgs.push(...findImages(val, depth + 1)));
                        }
                        return imgs;
                    };
                    const nuxtImages = findImages(window.__NUXT__.data);
                    if (nuxtImages.length > 0) return nuxtImages;
                }

                // –ü–∞—Ä—Å–∏–º <script> –¥–ª—è –ø–æ–∏—Å–∫–∞ JSON —Å –∫–∞—Ä—Ç–∏–Ω–∫–∞–º–∏
                const scripts = document.querySelectorAll('script');
                for (const script of scripts) {
                    const text = script.textContent;
                    if (!text) continue;
                    const urlMatches = text.matchAll(/https?:\/\/[^"'\s,\]]+\.(?:jpg|jpeg|png|webp)/gi);
                    for (const match of urlMatches) {
                        images.push(match[0]);
                    }
                }

                // –°–æ–±–∏—Ä–∞–µ–º –∏–∑ DOM (src –∏ data-–∞—Ç—Ä–∏–±—É—Ç—ã)
                document.querySelectorAll('img').forEach(img => {
                    if (img.src && !img.src.startsWith('data:')) images.push(img.src);
                    ['data-src', 'data-original', 'data-lazy-src'].forEach(attr => {
                        const val = img.getAttribute(attr);
                        if (val) images.push(val);
                    });
                });

                // –£–±–∏—Ä–∞–µ–º –¥—É–±–ª–∏–∫–∞—Ç—ã –∏ —Å–∏—Å—Ç–µ–º–Ω—ã–µ –∏–∫–æ–Ω–∫–∏
                return [...new Set(images)].filter(url =>
                    !url.includes('avatar') &&
                    !url.includes('logo') &&
                    !url.includes('icon') &&
                    !url.includes('button')
                );
            }
        """)

        # ‚ö° –ü—Ä–æ–∫—Ä—É—Ç–∫–∞, —á—Ç–æ–±—ã –ø–æ–¥–≥—Ä—É–∑–∏–ª–∏—Å—å –ª–µ–Ω–∏–≤—ã–µ –∫–∞—Ä—Ç–∏–Ω–∫–∏
        if not img_urls or len(img_urls) < 2:
            await page.evaluate("window.scrollTo(0, document.body.scrollHeight)")
            await asyncio.sleep(2)
            img_urls = await page.evaluate("""
                () => Array.from(document.querySelectorAll('img'))
                    .map(img => img.src)
                    .filter(u => u && !u.startsWith('data:'))
            """)

        return img_urls
    
    async def process_chapter_async(self, browser, chapter: Dict, ch_idx: int, manga_dir: str, download_images: bool = True) -> Dict:
        """–ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ –≥–ª–∞–≤—ã"""
        context = await browser.new_context(user_agent=HEADERS["User-Agent"])
        page = await context.new_page()
        page.set_default_timeout(30000)
        
        try:
            await page.goto(chapter['url'], wait_until='domcontentloaded')
            await asyncio.sleep(1)
            
            # –ë—ã—Å—Ç—Ä–æ–µ –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π
            img_urls = await self.extract_images_from_chapter(page)
            
            chapter_result = {
                **chapter,
                "chapter_id": f"{ch_idx}",
                "total_pages": len(img_urls),
                "pages": [],
                "download_status": "pending"
            }
            
            if not img_urls:
                chapter_result["download_status"] = "no_images"
                await context.close()
                return chapter_result
            
            # –°–æ–∑–¥–∞–µ–º –ø–∞–ø–∫—É –¥–ª—è –≥–ª–∞–≤—ã
            ch_dir = os.path.join(manga_dir, f"chapter_{ch_idx:03d}_{self.sanitize_filename(chapter['name'])}")
            
            if download_images:
                os.makedirs(ch_dir, exist_ok=True)
                
                # –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º —Å–ø–∏—Å–æ–∫ –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏
                download_list = []
                
                for idx, img_url in enumerate(img_urls, 1):
                    ext = "jpg"
                    if any(x in img_url.lower() for x in ['.png', '.webp', '.jpeg']):
                        ext = img_url.split('.')[-1].split('?')[0].lower()[:4]
                    
                    filename = os.path.join(ch_dir, f"page_{idx:03d}.{ext}")
                    # –¥–µ–ª–∞–µ–º –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω—ã–π –ø—É—Ç—å –æ—Ç –ø–∞–ø–∫–∏ manga
                    relative_path = os.path.relpath(filename, "manga").replace("\\", "/")
                    # —Ç–µ–ø–µ—Ä—å —Ñ—Ä–æ–Ω—Ç –±—É–¥–µ—Ç –ø–æ–ª—É—á–∞—Ç—å /static/...
                    chapter_result["pages"].append(f"/static/{relative_path}")
                    download_list.append((img_url, filename))

                # –ó–∞–≥—Ä—É–∂–∞–µ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –∞—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–æ
                downloaded = await self.download_images_batch(download_list)
                chapter_result["download_status"] = "completed" if downloaded > 0 else "failed"
            else:
                # –ü—Ä–æ—Å—Ç–æ —Å–æ—Ö—Ä–∞–Ω—è–µ–º URL –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π
                chapter_result["pages"] = img_urls
                chapter_result["download_status"] = "urls_only"
            
            await context.close()
            return chapter_result
            
        except Exception as e:
            print(f"[ERROR] –û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ –≥–ª–∞–≤—ã {chapter['name']}: {e}")
            chapter_result["download_status"] = "error"
            chapter_result["error"] = str(e)
            await context.close()
            return chapter_result
    
    async def get_manga_info(self, url: str, max_chapters: Optional[int] = None) -> Dict:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ –º–∞–Ω–≥–µ —Å –∑–∞–≥—Ä—É–∑–∫–æ–π –ø–µ—Ä–≤—ã—Ö –≥–ª–∞–≤ –∏ –∫–∞—Ä—Ç–∏–Ω–æ–∫"""
        browser = await browser_pool.chromium.launch(
            headless=True,
            args=[
                '--disable-blink-features=AutomationControlled',
                '--disable-dev-shm-usage',
                '--no-sandbox',
            ]
        )

        def fix_page_url(page_url: str) -> str:
            """–ò—Å–ø—Ä–∞–≤–ª—è–µ–º –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω—ã–µ –ø—É—Ç–∏ –Ω–∞ –ø–æ–ª–Ω—ã–µ —Å—Å—ã–ª–∫–∏"""
            if page_url.startswith("http"):
                return page_url
            return f"{BASE_URL}{page_url}"

        try:
            context = await browser.new_context(
                user_agent=HEADERS["User-Agent"],
                viewport={"width": 1920, "height": 1080}
            )

            page = await context.new_page()
            page.set_default_timeout(30000)

            print(f"–ü–µ—Ä–µ—Ö–æ–¥–∏–º –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü—É: {url}")

            try:
                await page.goto(url, wait_until='domcontentloaded')
            except Exception as e:
                print(f"–ü—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏–µ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ —Å—Ç—Ä–∞–Ω–∏—Ü—ã: {e}")

            # –ü–æ–ª—É—á–∞–µ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –º–∞–Ω–≥–∏
            manga_info = await self.get_full_manga_info(page)
            manga_info["source_url"] = url
            manga_info["manga_id"] = self.get_manga_id(url)

            # –°–æ–∑–¥–∞—ë–º —Å—Ç—Ä—É–∫—Ç—É—Ä—É –ø–∞–ø–æ–∫
            manga_dir = os.path.join("manga", self.sanitize_filename(manga_info["title"]))
            covers_dir = os.path.join(manga_dir, "covers")
            os.makedirs(covers_dir, exist_ok=True)

            # –°–∫–∞—á–∏–≤–∞–µ–º –æ–±–ª–æ–∂–∫—É
            if manga_info.get("cover_url") and not manga_info["cover_url"].startswith("data:"):
                cover_path = os.path.join(covers_dir, "main_cover.jpg")
                cover_url = urljoin(BASE_URL, manga_info["cover_url"]) if manga_info["cover_url"].startswith("/") else manga_info["cover_url"]

                try:
                    print(f"–°–∫–∞—á–∏–≤–∞–µ–º –æ–±–ª–æ–∂–∫—É: {cover_url}")
                    r = requests.get(cover_url, headers={**HEADERS, "Referer": BASE_URL}, timeout=30)
                    r.raise_for_status()
                    with open(cover_path, "wb") as f:
                        f.write(r.content)
                    manga_info["local_cover_path"] = cover_path
                    print(f"‚úÖ –û–±–ª–æ–∂–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞: {cover_path}")
                except Exception as e:
                    print(f"[WARN] –ù–µ —É–¥–∞–ª–æ—Å—å —Å–∫–∞—á–∞—Ç—å –æ–±–ª–æ–∂–∫—É: {e}")

            # –ü–æ–ª—É—á–∞–µ–º —Å–ø–∏—Å–æ–∫ –≥–ª–∞–≤
            chapters = await page.evaluate("""
                () => {
                    const chapters = [];
                    const links = document.querySelectorAll('a[href*="/reader/"]');
                    links.forEach((link, index) => {
                        const href = link.getAttribute('href');
                        if (href && href.includes('/reader/')) {
                            chapters.push({
                                name: link.textContent.trim() || '–ì–ª–∞–≤–∞ –±–µ–∑ –Ω–∞–∑–≤–∞–Ω–∏—è',
                                url: href.startsWith('http') ? href : window.location.origin + href,
                                chapter_id: (index + 1).toString()
                            });
                        }
                    });
                    return chapters;
                }
            """)

            print(f"üìö –ù–∞–π–¥–µ–Ω–æ {len(chapters)} –≥–ª–∞–≤")

            if max_chapters:
                chapters = chapters[:max_chapters]
                print(f"üìñ –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –ø–µ—Ä–≤—ã–µ {max_chapters} –≥–ª–∞–≤")

            # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –≥–ª–∞–≤—ã (—Å –∫–∞—Ä—Ç–∏–Ω–∫–∞–º–∏)
            manga_info["chapters"] = []
            for idx, chapter in enumerate(chapters, start=1):
                try:
                    chapter_result = await self.process_chapter_async(
                        browser,
                        chapter,
                        idx,
                        manga_dir,
                        download_images=False   # ‚ö° —Ç–æ–ª—å–∫–æ —Å—Å—ã–ª–∫–∏, –±–µ–∑ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è
                    )

                    # ‚úÖ —Ñ–∏–∫—Å–∏—Ä—É–µ–º —Å—Å—ã–ª–∫–∏ –∫–∞—Ä—Ç–∏–Ω–æ–∫
                    chapter_result["pages"] = [fix_page_url(p) for p in chapter_result["pages"]]

                    manga_info["chapters"].append(chapter_result)
                    print(f"‚úÖ –ì–ª–∞–≤–∞ {chapter_result['name']} –∑–∞–≥—Ä—É–∂–µ–Ω–∞ ({chapter_result['total_pages']} —Å—Ç—Ä.)")
                except Exception as e:
                    print(f"[ERROR] –ù–µ —É–¥–∞–ª–æ—Å—å –æ–±—Ä–∞–±–æ—Ç–∞—Ç—å –≥–ª–∞–≤—É {chapter['name']}: {e}")

            manga_info["total_chapters"] = len(manga_info["chapters"])

            # –°–æ—Ö—Ä–∞–Ω—è–µ–º JSON –ª–æ–∫–∞–ª—å–Ω–æ
            try:
                json_path = os.path.join(manga_dir, "manga_info.json")
                with open(json_path, "w", encoding="utf-8") as f:
                    json.dump(manga_info, f, ensure_ascii=False, indent=2)
                print(f"üíæ –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞: {json_path}")
            except Exception as e:
                print(f"[WARN] –ù–µ —É–¥–∞–ª–æ—Å—å —Å–æ—Ö—Ä–∞–Ω–∏—Ç—å JSON: {e}")

            await context.close()
            return manga_info

        finally:
            await browser.close()

# –°–æ–∑–¥–∞–µ–º —ç–∫–∑–µ–º–ø–ª—è—Ä –ø–∞—Ä—Å–µ—Ä–∞
parser = FastMangaParser(max_workers=10)

@app.get("/", summary="–ì–ª–∞–≤–Ω–∞—è —Å—Ç—Ä–∞–Ω–∏—Ü–∞")
async def root():
    return {
        "message": "Manga Parser API",
        "endpoints": {
            "manga_info": "/manga?url=<manga_url>&max_chapters=<number>",
            "chapter_download": "/chapters/{chapter_id}?manga_url=<url>"
        },
        "example": {
            "manga_info": "/manga?url=https://webfandom.ru/publications/manga-vseveduschij-chitatel",
            "chapter_download": "/chapters/1?manga_url=https://webfandom.ru/publications/manga-vseveduschij-chitatel"
        }
    }

@app.get("/manga", response_model=MangaResponse, summary="–ü–æ–ª—É—á–∏—Ç—å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –º–∞–Ω–≥–µ")
async def get_manga_info_endpoint(
    url: str = Query(..., description="URL –º–∞–Ω–≥–∏ —Å webfandom.ru"),
    max_chapters: Optional[int] = Query(None, description="–ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –≥–ª–∞–≤ –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏")
):
    """
    –ü–æ–ª—É—á–∞–µ—Ç –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –º–∞–Ω–≥–∏ –ø–æ URL:
    - –ù–∞–∑–≤–∞–Ω–∏–µ –∏ –∞–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω—ã–µ –Ω–∞–∑–≤–∞–Ω–∏—è
    - –û–ø–∏—Å–∞–Ω–∏–µ, –∂–∞–Ω—Ä—ã, —Ç–µ–≥–∏
    - –°–ø–∏—Å–æ–∫ –≤—Å–µ—Ö –≥–ª–∞–≤
    - –û–±–ª–æ–∂–∫–∞
    - –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è
    """
    if not url.startswith("https://webfandom.ru"):
        raise HTTPException(status_code=400, detail="URL –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å —Å —Å–∞–π—Ç–∞ webfandom.ru")
    
    manga_id = parser.get_manga_id(url)
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫–µ—à
    if manga_id in manga_cache:
        cached_data = manga_cache[manga_id]
        print(f"üìã –í–æ–∑–≤—Ä–∞—â–∞–µ–º –¥–∞–Ω–Ω—ã–µ –∏–∑ –∫–µ—à–∞ –¥–ª—è {cached_data['title']}")
        return cached_data
    
    try:
        print(f"üîç –ü–æ–ª—É—á–µ–Ω–∏–µ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ –º–∞–Ω–≥–µ: {url}")
        manga_info = await parser.get_manga_info(url, max_chapters)
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ –∫–µ—à
        manga_cache[manga_id] = manga_info
        
        return manga_info
        
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–∞—Ä—Å–∏–Ω–≥–µ: {str(e)}")

@app.get("/chapters/{chapter_id}", response_model=ChapterResponse, summary="–ó–∞–≥—Ä—É–∑–∏—Ç—å –∫–æ–Ω–∫—Ä–µ—Ç–Ω—É—é –≥–ª–∞–≤—É")
async def download_chapter(
    chapter_id: str,
    manga_url: str = Query(..., description="URL –º–∞–Ω–≥–∏"),
    download_images: bool = Query(True, description="–ó–∞–≥—Ä—É–∂–∞—Ç—å –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –∏–ª–∏ —Ç–æ–ª—å–∫–æ URL")
):
    """
    –ó–∞–≥—Ä—É–∂–∞–µ—Ç –∫–æ–Ω–∫—Ä–µ—Ç–Ω—É—é –≥–ª–∞–≤—É –ø–æ ID:
    - –ü–æ–ª—É—á–∞–µ—Ç –≤—Å–µ —Å—Ç—Ä–∞–Ω–∏—Ü—ã –≥–ª–∞–≤—ã
    - –û–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ —Å–∫–∞—á–∏–≤–∞–µ—Ç –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –Ω–∞ —Å–µ—Ä–≤–µ—Ä
    - –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –ø—É—Ç–∏ –∫ —Ñ–∞–π–ª–∞–º –∏–ª–∏ URL –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π
    """
    if not manga_url.startswith("https://webfandom.ru"):
        raise HTTPException(status_code=400, detail="URL –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å —Å —Å–∞–π—Ç–∞ webfandom.ru")
    
    manga_id = parser.get_manga_id(manga_url)

    def fix_page_url(page_url: str) -> str:
        """–ò—Å–ø—Ä–∞–≤–ª—è–µ–º –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω—ã–µ –ø—É—Ç–∏ –Ω–∞ –ø–æ–ª–Ω—ã–µ —Å—Å—ã–ª–∫–∏"""
        if page_url.startswith("http"):
            return page_url
        return f"{BASE_URL}{page_url}"
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –µ—Å—Ç—å –ª–∏ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –º–∞–Ω–≥–µ –≤ –∫–µ—à–µ
    manga_info = manga_cache.get(manga_id)
    if not manga_info:
        # –ï—Å–ª–∏ –Ω–µ—Ç –≤ –∫–µ—à–µ, –ø–æ–ª—É—á–∞–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é
        try:
            manga_info = await parser.get_manga_info(manga_url)
            manga_cache[manga_id] = manga_info
        except Exception as e:
            raise HTTPException(status_code=500, detail=f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–ª—É—á–µ–Ω–∏–∏ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ –º–∞–Ω–≥–µ: {str(e)}")
    
    # –ù–∞—Ö–æ–¥–∏–º –≥–ª–∞–≤—É –ø–æ ID
    chapter_to_download = None
    for chapter in manga_info["chapters"]:
        if chapter.get("chapter_id") == chapter_id:
            chapter_to_download = chapter
            break
    
    if not chapter_to_download:
        raise HTTPException(status_code=404, detail=f"–ì–ª–∞–≤–∞ —Å ID {chapter_id} –Ω–µ –Ω–∞–π–¥–µ–Ω–∞")
    
    try:
        print(f"üìñ –ó–∞–≥—Ä—É–∑–∫–∞ –≥–ª–∞–≤—ã {chapter_id}: {chapter_to_download['name']}")
        
        manga_dir = os.path.join("manga", parser.sanitize_filename(manga_info["title"]))
        
        browser = await browser_pool.chromium.launch(headless=True, args=['--no-sandbox'])
        
        try:
            chapter_result = await parser.process_chapter_async(
                browser, 
                chapter_to_download, 
                int(chapter_id), 
                manga_dir, 
                download_images
            )

            # ‚úÖ —Ñ–∏–∫—Å–∏—Ä—É–µ–º –≤—Å–µ —Å—Å—ã–ª–∫–∏ –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü—ã
            chapter_result["pages"] = [fix_page_url(p) for p in chapter_result["pages"]]
            
            return ChapterResponse(
                chapter_id=chapter_result["chapter_id"],
                name=chapter_result["name"],
                pages=chapter_result["pages"],
                total_pages=chapter_result["total_pages"],
                download_status=chapter_result["download_status"]
            )
            
        finally:
            await browser.close()
            
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ –≥–ª–∞–≤—ã: {str(e)}")

@app.get("/health", summary="–ü—Ä–æ–≤–µ—Ä–∫–∞ —Å–æ—Å—Ç–æ—è–Ω–∏—è —Å–µ—Ä–≤–µ—Ä–∞")
async def health_check():
    """–ü—Ä–æ—Å—Ç–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ —Å–æ—Å—Ç–æ—è–Ω–∏—è —Å–µ—Ä–≤–µ—Ä–∞"""
    return {
        "status": "healthy",
        "cached_manga": len(manga_cache),
        "message": "–°–µ—Ä–≤–µ—Ä —Ä–∞–±–æ—Ç–∞–µ—Ç –Ω–æ—Ä–º–∞–ª—å–Ω–æ"
    }

if __name__ == "__main__":
    print("üöÄ –ó–∞–ø—É—Å–∫ FastAPI —Å–µ—Ä–≤–µ—Ä–∞ –¥–ª—è –ø–∞—Ä—Å–∏–Ω–≥–∞ –º–∞–Ω–≥–∏")
    print("üìö –î–æ—Å—Ç—É–ø–Ω—ã–µ —ç–Ω–¥–ø–æ–∏–Ω—Ç—ã:")
    print("   GET /manga?url=<url> - –ü–æ–ª—É—á–∏—Ç—å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –º–∞–Ω–≥–µ")
    print("   GET /chapters/{id}?manga_url=<url> - –ó–∞–≥—Ä—É–∑–∏—Ç—å –≥–ª–∞–≤—É")
    print("   GET /health - –ü—Ä–æ–≤–µ—Ä–∫–∞ —Å–æ—Å—Ç–æ—è–Ω–∏—è")
    print("üåê Swagger UI: http://localhost:8000/docs")
    
    uvicorn.run("server:app", host="0.0.0.0", port=8000, reload=True)